{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342e0907-6c05-422d-b74b-71b85bf1081d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2\n",
    "!pip install -e .\n",
    " \n",
    "!pip install pydantic-settings\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "# Restart the Python process to use the updated packages\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c286d603-2ec5-45bd-bcd6-f98143f5b07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration, T5TokenizerFast, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6145960-7b8b-4465-bd38-d435801d1a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.llm_research.inference import run_inference, run_inference_manual\n",
    "from src.llm_research.evaluation import calculate_token_level_match_accuracy, calculate_exact_match_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b74b3c9-bc58-4f11-81ee-81154771ba5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Reimer.constants import(\n",
    "    MODEL_MAPPING,\n",
    "    EVAL_LANGUAGES\n",
    ")\n",
    "STORE_RESULTS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8762b54f-05fe-466a-89dc-51d03e57c5c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, get_peft_model\n",
    "from typing import Dict\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    BitsAndBytesConfig,\n",
    "    MaxLengthCriteria,\n",
    "    AutoConfig,\n",
    "    MT5ForConditionalGeneration,\n",
    ")\n",
    "try:\n",
    "    import flash_attn\n",
    "\n",
    "    flash_attn_available = True\n",
    "except ImportError:\n",
    "    flash_attn_available = False\n",
    "    print(\"flash_attn not installed. Using default attention implementation.\")\n",
    "\n",
    "\n",
    "class StopAtLineEndCriterion(StoppingCriteria):\n",
    "    \"\"\"A stopping criterion that halts the text generation when a newline character is encountered.\n",
    "\n",
    "    This stopping criterion is useful when the generation of text should stop\n",
    "    at the end of a line, such as for tasks where each output should be a single line of text.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer: The tokenizer that is used to decode the IDs back to text.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: An instance of a tokenizer that converts token IDs to text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"Checks if the generation should be stopped.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tensor of token IDs representing the current state of generated text.\n",
    "            scores: The generation scores for the current set of tokens.\n",
    "\n",
    "        Returns:\n",
    "            A boolean indicating if the newline character is at the end of the text, thus\n",
    "            generation should stop.\n",
    "        \"\"\"\n",
    "        # Convert token IDs to text\n",
    "        text = self.tokenizer.decode(input_ids[0])\n",
    "        # Check if a newline character is in the text\n",
    "        return text.endswith(\"\\n\")\n",
    "    \n",
    "        import flash_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc90d1af-8b6c-4628-9cb6-31a5faf1d605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def call_run_inference(\n",
    "    model,\n",
    "    model_path,\n",
    "    tokenizer,\n",
    "    task,\n",
    "    language,\n",
    "    is_peft,\n",
    "    token_level_eval,\n",
    "    generation_config,\n",
    "    is_qlora=None,\n",
    "    subset=None\n",
    "):\n",
    "    if subset:\n",
    "        dataset_test_path = Path(f\"/dbfs/Reimer/data/{task}/{task}_{language}_validation-filtered-samples{subset}.json\")\n",
    "    else:\n",
    "        dataset_test_path = Path(f\"/dbfs/Reimer/data/{task}/{task}_{language}_validation.json\")\n",
    "\n",
    "    pred_file_path = Path(f\"{Path(model_path).name}-{task}-{language}-predictions.json\")\n",
    "\n",
    "    run_inference_manual(\n",
    "        dataset_path=dataset_test_path,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_id=model_path,\n",
    "        is_peft=is_peft,\n",
    "        is_qlora=is_qlora,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        generation_config=generation_config,\n",
    "        out_path=pred_file_path,\n",
    "        dataset_split='train',\n",
    "    )\n",
    "\n",
    "    if token_level_eval:\n",
    "        #calculate token level accuracy\n",
    "        exact_match_accuracy, mismatch_percentage = calculate_token_level_match_accuracy(\n",
    "            ground_truth_file=dataset_test_path,\n",
    "            prediction_file=pred_file_path,\n",
    "        )\n",
    "    else:\n",
    "        # Compute exact match accuracy\n",
    "        exact_match_accuracy = calculate_exact_match_accuracy(\n",
    "            ground_truth_file=dataset_test_path,\n",
    "            prediction_file=pred_file_path,\n",
    "        )\n",
    "        mismatch_percentage = None\n",
    "    return exact_match_accuracy, mismatch_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f3b2f9-4e3a-4b9f-afcd-41b0f1b22ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_task_lang_combos(\n",
    "    model_id,\n",
    "    tokenizer_path,\n",
    "    generation_config,\n",
    "    tasks,\n",
    "    langs,\n",
    "    is_qlora=False,\n",
    "    device=\"cuda\",\n",
    "    file_count = 1,\n",
    "    skip_existing = False,\n",
    "    subset_wiki=None,\n",
    "    use_existing=True,\n",
    "    save=True\n",
    "):  \n",
    "    print(f\"Saving results: {save}\")\n",
    "\n",
    "    #creating results dict\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(\"Bigscience/mt0-large\")\n",
    "    # model = MT5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    results_dict = {\n",
    "        'model' : str(model_path),\n",
    "        'tokenizer' : str(tokenizer_path)\n",
    "    }\n",
    "    results_dict['subset wikiann'] = subset_wiki\n",
    "    generation_config[\"eos_token_id\"] = tokenizer.eos_token_id,\n",
    "    results_dict['config'] = generation_config\n",
    "\n",
    "    # saving results dict\n",
    "    results_dict_path = Path(f\"accuracies/{Path(model_path).name}_accuracies.json\")\n",
    "    results_dict_stem = Path(f\"accuracies/{Path(model_path).name}_accuracies\")\n",
    "\n",
    "    if results_dict_stem.exists():      #ugly but quickest fix\n",
    "        results_dict_path = results_dict_stem\n",
    "\n",
    "    if results_dict_path.exists() and use_existing:\n",
    "        print(f\"Found existing results dict at {results_dict_path}, using existing.\")\n",
    "        with open(results_dict_path, 'r', encoding='utf-8') as f:\n",
    "            results_dict = json.load(f)\n",
    "            new_tasks = [task for task in tasks if task not in results_dict.keys()]\n",
    "            print(f\"New tasks: {new_tasks}\")\n",
    "            tasks = new_tasks\n",
    "    else:\n",
    "        while(results_dict_path.exists() or results_dict_stem.exists()):\n",
    "            print(f\"Found existing file: {results_dict_path}\")\n",
    "            if skip_existing:\n",
    "                print(f\"Skipping {results_dict_path}\")\n",
    "                return 0\n",
    "            else:\n",
    "                results_dict_path = Path(f\"accuracies/{Path(model_path).name}_accuracies{str(file_count)}.json\")\n",
    "                file_count += 1\n",
    "\n",
    "    if len(tasks) == 0:\n",
    "        print(f\"No tasks\")\n",
    "        return 0\n",
    "\n",
    "    if results_dict_path is not None and save:\n",
    "        with open(results_dict_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_dict, f, indent=4)\n",
    "\n",
    "    if \"lora\" in str(model_path):\n",
    "        is_peft = True\n",
    "    else:\n",
    "        is_peft = False\n",
    "\n",
    "    #load and configure model / tokenizer\n",
    "    # configure the model\n",
    "    if is_peft:\n",
    "        print(\"Converting to PEFT model...\")\n",
    "        config = PeftConfig.from_pretrained(model_id)\n",
    "        inference_model_id = config.base_model_name_or_path\n",
    "        is_encoder_decoder = config.task_type == \"SEQ_2_SEQ_LM\"\n",
    "    else:\n",
    "        inference_model_id = model_id\n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "        is_encoder_decoder = config.is_encoder_decoder\n",
    "\n",
    "    bnb_config = None\n",
    "    if is_qlora:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\n",
    "            inference_model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        inference_model_id,\n",
    "        padding_side=\"left\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if is_peft:\n",
    "        # add the adapters to the base model\n",
    "        model = PeftModel.from_pretrained(model, model_id)\n",
    "        if not is_qlora:\n",
    "            model = model.merge_and_unload()\n",
    "\n",
    "    # Initialize stopping criteria - stop at new line char\n",
    "    stop_criterium = StoppingCriteriaList(\n",
    "        [\n",
    "            StopAtLineEndCriterion(tokenizer=tokenizer),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for task in tasks:\n",
    "        token_level_eval = False\n",
    "\n",
    "        if task == 'wikiann':\n",
    "            token_level_eval = True\n",
    "            subset = subset_wiki\n",
    "        else:\n",
    "            subset = None\n",
    "\n",
    "        print(f\"subset for {task} is {subset}\")\n",
    "        \n",
    "        if langs is None:\n",
    "            langs = EVAL_LANGUAGES[task]\n",
    "            \n",
    "        for language in langs:\n",
    "            exact_match_accuracy, mismatch_count = call_run_inference(\n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                task=task, \n",
    "                language=language,           \n",
    "                token_level_eval=token_level_eval, \n",
    "                model_path=model_path, \n",
    "                is_peft=is_peft, \n",
    "                generation_config=generation_config,\n",
    "                subset=subset\n",
    "            )\n",
    "\n",
    "            print(f\"Mismatch count for {Path(model_path).name} - {task} - {language} : {mismatch_count}\")\n",
    "\n",
    "            if results_dict_path is not None:\n",
    "                with open(results_dict_path, 'r', encoding='utf-8') as f:\n",
    "                    results_dict = json.load(f)\n",
    "\n",
    "                if not task in results_dict.keys():\n",
    "                    results_dict[task] = {}\n",
    "                if not language in results_dict.keys():\n",
    "                    results_dict[task][language] = {}\n",
    "\n",
    "                results_dict[task][language]['token_level_eval'] = token_level_eval\n",
    "                results_dict[task][language]['accuracy'] = exact_match_accuracy\n",
    "\n",
    "                if save:\n",
    "                    with open(results_dict_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(results_dict, f, indent=4)\n",
    "\n",
    "            print(f\"Accuracy for {Path(model_path).name} - {task} - {language} : {exact_match_accuracy} (token_level = {token_level_eval})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaea573-7a0f-4763-8525-a860ea239f74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_dir = dbutils.fs.ls(\"dbfs:/Reimer/merged_models/mono\")\n",
    "model_list = [f'/{str(p.path).replace(\":\", \"\")}' for p in model_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c558b25c-28e5-4aa6-b424-f7a9e59c20b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LEN = 512\n",
    "TEMPERATURE = 0.9\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "NUM_RETURN_SEQ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7666fb-7f9d-4d2c-922e-0053e6fe9d79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_model_names(\n",
    "    task2,\n",
    "    lang2,\n",
    "    merge_method,\n",
    "    base = None,\n",
    "    task1 = None,\n",
    "    lang1 = None,\n",
    "    ft1 = \"ft\",\n",
    "    ft2 = \"ft\",\n",
    "    bm1 = \"mt0-large\",\n",
    "    bm2 = \"mt0-large\",\n",
    "    base_path = \"/dbfs/Reimer/merged_models\",\n",
    "    connecter = \"--\"\n",
    "):\n",
    "    assert base or (task1 and lang1), \"Need base or finetuned model for model1\"\n",
    "\n",
    "    if base:\n",
    "        model1 = bm1\n",
    "    else:\n",
    "        model1 = f\"{bm1}_{task1}_{ft1}_{lang1}\"\n",
    "\n",
    "    model2 = f\"{bm2}_{task2}_{ft2}_{lang2}\"\n",
    "\n",
    "    return f\"{base_path}/{merge_method}/{model1}{connecter}{model2}-{merge_method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264d1413-e2b5-4518-8d06-a15ac5483b11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "languages = ['ar', 'de', 'el', 'es']\n",
    "base = 'mt0-large'\n",
    "tasks = ['wikiann']\n",
    "merging_methods = ['mono', 'pooling']\n",
    "\n",
    "base_list = [\n",
    "    create_model_names(\n",
    "        task2=task2,\n",
    "        lang2=lang2,\n",
    "        merge_method=merge_method,\n",
    "        base=True\n",
    "    )\n",
    "    for task2 in tasks\n",
    "    for merge_method in merging_methods\n",
    "    for lang2 in languages\n",
    "]\n",
    "\n",
    "print(\"\".join([f\"{model}\\n\" for model in base_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c75df4ef-0789-4801-a703-13856a915474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "task = \"xnli\"\n",
    "output_dir = f\"/dbfs/Reimer/output/{task}\"\n",
    "\n",
    "xnli_list = [f\"{output_dir}/experiment_mt0-large_{task}_ft_{lang}/\" for lang in languages]\n",
    "xnli_list = xnli_list[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fd498a-76d4-4d0f-ada5-b4dfa1cd331e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, combinations\n",
    "\n",
    "languages = ['ar', 'de', 'el', 'es']\n",
    "task1 = 'sib200'\n",
    "task2 = 'sib200'\n",
    "# merging_methods = ['mono', 'pooling']\n",
    "merging_methods = ['mono']\n",
    "\n",
    "wt_model_list_sib = [\n",
    "    create_model_names(\n",
    "        task1=task1,\n",
    "        lang1=lang1,\n",
    "        task2=task2,\n",
    "        lang2=lang2,\n",
    "        merge_method=merge_method\n",
    "    )\n",
    "    for merge_method in merging_methods\n",
    "    for lang1, lang2 in combinations(languages, 2)\n",
    "]\n",
    "\n",
    "print(\"\".join([f\"{model}\\n\" for model in wt_model_list_sib]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d72d9f01-6794-4c0b-bca7-ef244380404b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, combinations\n",
    "\n",
    "languages = ['ar', 'de', 'el', 'es']\n",
    "task1 = 'sib200'\n",
    "task2 = 'xnli'\n",
    "# merging_methods = ['mono', 'pooling']\n",
    "merging_methods = ['mono']\n",
    "\n",
    "ct_model_list_sib_xnli = [\n",
    "    create_model_names(\n",
    "        task1=task1,\n",
    "        lang1=lang1,\n",
    "        task2=task2,\n",
    "        lang2=lang2,\n",
    "        merge_method=merge_method\n",
    "    )\n",
    "    for merge_method in merging_methods\n",
    "    for lang1, lang2 in combinations_with_replacement(languages, 2)\n",
    "]\n",
    "\n",
    "print(\"\".join([f\"{model}\\n\" for model in ct_model_list_sib_xnli]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8590d5-cf40-410a-b186-268d597b1054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = f\"/dbfs/Reimer/output/sib200\"\n",
    "task = \"sib200\"\n",
    "\n",
    "sib_list = [f\"{output_dir}/experiment_mt0-large_{task}_ft_{lang}\" for lang in languages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5419ecf2-4bc1-46ff-8936-e763873f0e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "tokenizer_path = f\"{model_path}/tokenizer_config.json\"\n",
    "generation_config = {\n",
    "        \"max_length\": MAX_SEQ_LEN,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"num_return_sequences\": NUM_RETURN_SEQ,\n",
    "    }\n",
    "\n",
    "tasks = ['xnli', 'sib200', 'wikiann']\n",
    "\n",
    "subset = 500          #only used in case of wikiann\n",
    "\n",
    "model_list = ct_model_list_sib_xnli[:5]\n",
    "\n",
    "for model_path in model_list: \n",
    "    print(f\"Testing for {model_path}\")\n",
    "    run_task_lang_combos(\n",
    "        model_id=model_path, \n",
    "        tokenizer_path = tokenizer_path, \n",
    "        tasks=tasks, \n",
    "        langs=None, \n",
    "        generation_config=generation_config, \n",
    "        skip_existing=False,\n",
    "        subset_wiki=subset,\n",
    "        use_existing=True,\n",
    "        save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb447be0-e8d5-4e47-83ae-939e7468bae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def subsample_data(\n",
    "    data_path,\n",
    "    N,\n",
    "    seed = 34\n",
    "):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    random.seed(seed)\n",
    "    data = random.choices(data, k=500)\n",
    "    \n",
    "    new_path = data_path.split(\".json\")[0] + f\"-samples{N}\" + \".json\"\n",
    "    print(f\"Saving subsampled data under {new_path}\")\n",
    "\n",
    "    with open(new_path, 'wt', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90ffe79-a66b-4eb9-90d2-a2b8f3078c21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data_list = [p.path for p in dbutils.fs.ls(\"dbfs:/Reimer/data/wikiann\") if 'filtered' in p.name and 'samples' not in p.name]\n",
    "\n",
    "# for p in data_list:\n",
    "#     subsample_data(f\"/{p.replace(':', '')}\", N=500, seed=32)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "run_inference",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
